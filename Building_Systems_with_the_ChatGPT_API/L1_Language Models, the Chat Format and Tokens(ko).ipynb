{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 ì–¸ì–´ ëª¨ë¸, ì±„íŒ… í˜•ì‹, ê·¸ë¦¬ê³  í† í°\n",
    "## Setup\n",
    "#### API keyì™€ ê´€ë ¨ëœ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "ë³¸ ê°•ì˜ëŠ” ë‹¹ì‹ ì„ ìœ„í•œ OpenAI API keyë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.  \n",
    "(ê³µì§œë¼ëŠ” ëœ» ğŸ’°âŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ ë°©ì‹ì²˜ëŸ¼ ì—¬ëŸ¬ë¶„ì˜ API keyë¥¼ ì§ì ‘ ë³€ìˆ˜ì— í• ë‹¹í•˜ì§€ ì•Šê³  ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ì´ ê¶Œì¥ë©ë‹ˆë‹¤.  \n",
    "<span style='color:red'>ë‹¨ìˆœíˆ íŒŒì¼ì´ ê³µìœ ë˜ê±°ë‚˜ ê¹ƒí—ˆë¸Œì— ì—…ë¡œë“œë˜ëŠ” ê²ƒë§Œìœ¼ë¡œ API keyê°€ ìœ ì¶œë˜ì–´ í° í”¼í•´ë¥¼ ì…ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸</span>ì…ë‹ˆë‹¤.  \n",
    "ê¼­ ì €ëŸ° íŒŒì¼ì´ ì•„ë‹ˆì–´ë„ ê´œì°®ìœ¼ë‹ˆ í¸í•œëŒ€ë¡œ ì“°ë©´ ë©ë‹ˆë‹¤.  \n",
    "ì €ê°™ì€ ê²½ìš°ëŠ” txt íŒŒì¼ì— í‚¤ë¥¼ ì €ì¥í•´ë‘ê³  ë¶ˆëŸ¬ì™€ì„œ ì¼ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "ì•„ë§ˆ \"ChatGPT Prompt Engineering for Developers\" ê°•ì˜ë¥¼ ë“¤ìœ¼ì…¨ë‹¤ë©´ ìµìˆ™í•œ ê¸°ëŠ¥ì´ê² ë„¤ìš”!  \n",
    "ì£¼ì–´ì§„ ì…ë ¥ ë‹¤ìŒì„ ì™„ì„±í•˜ëŠ” ì¶œë ¥ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ì—ê²Œ ì§ˆë¬¸í•˜ê³  ì™„ì„±ëœ ë¬¸ì¥ ë°˜í™˜ë°›ê¸°\n",
    "\n",
    "ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  \"What is the capital of France?\" ë¼ëŠ” ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ì£¼ì—ˆìŠµë‹ˆë‹¤.  \n",
    "ì´ì— ëŒ€í•œ <span style='color:red'>ëª¨ë¸ì˜ ë‹µë³€ì´ responseì— ì €ì¥</span>ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í† í°\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œë³´ë©´ ìƒê°ë³´ë‹¤ ê°„ë‹¨í•´ ë³´ì´ëŠ” ì´ íƒœìŠ¤í¬ë¥¼ ëª¨ë¸ì´ ì œëŒ€ë¡œ ìˆ˜í–‰í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì´ëŠ” ëª¨ë¸ì´ ì–¸ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ë•Œ, í•™ìŠµí–ˆë˜ ê²ƒë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìª¼ê°œì–´ í† í°í™”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.  \n",
    "ì˜ˆë¥¼ ë“¤ë©´ lollipopì€ lol,li,pop ë“±ìœ¼ë¡œ ìª¼ê°œì§€ê¸° ë•Œë¬¸ì— <span style='color:red'>ìš°ë¦¬ì˜ ì˜ë„ëŒ€ë¡œ ëª¨ë¸ì´ ì…ë ¥ì„ ë°›ì§€ ëª»í•˜ëŠ” ê²ƒ</span>ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"lollipop\" ì„ ê±°ê¾¸ë¡œí•˜ë©´ \"popillol\" ì´ë ‡ê²Œ ë˜ì–´ì•¼ê² ì£ .  \n",
    "ë”°ë¼ì„œ ì…ë ¥ì´ ì œëŒ€ë¡œ êµ¬ë¶„ë  ìˆ˜ ìˆë„ë¡ ì•„ë˜ì™€ ê°™ì´ ì…ë ¥ì„ ì œê³µí•˜ë©´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function (ì±„íŒ… í˜•íƒœ)\n",
    "ì—¬ê¸°ì— ìš°ë¦¬ê°€ ê°•ì˜ì—ì„œ ì‚¬ìš©í•  ìƒˆë¡œìš´ helper functionì´ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì´ì „ê³¼ ë‹¤ë¥´ê²Œ messagesë¼ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•˜ì—¬ <span style='color:red'>ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê²Œ ëœë‹¤</span>ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roleì„ ë³´ë©´ <span style='color:red'>systemì€ ë§ ê·¸ëŒ€ë¡œ ì‹œìŠ¤í…œì ì¸ ì„¸íŒ…ì´ê³ , userëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥</span>ì„ ëœ»í•˜ê²Œ ë©ë‹ˆë‹¤.  \n",
    "ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” assistant ì—­í• ê³¼, í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ë¼ëŠ” ì„¸íŒ… ë‘ ê°œë¥¼ ê°ê° ì‚¬ìš©í•œ ê²½ìš°ì™€ ë‘˜ì„ í•©ì¹œ ê²½ìš°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ ë°˜í™˜í•˜ëŠ” ê²°ê³¼ë¥¼ ë‹´ëŠ” ë³€ìˆ˜ responseì˜ ê°’ë“¤ì„ êº¼ë‚´ì–´ ë³´ë©´ ì‚¬ìš©ëœ í† í°ì˜ ê°œìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì¼ë°˜ì ìœ¼ë¡œ GPTëŠ” ì…ë ¥ì„ ìµœëŒ€ 4,000 í† í° ê¸¸ì´ë¡œ ë°›ìœ¼ë©°, ì¶œë ¥ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í† í°ì˜ ê°œìˆ˜ ë‹¨ìœ„ë¡œ ê¸ˆì•¡ì„ ì§€ë¶ˆí•˜ê²Œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì§§ì€ ë¬¸ì¥ë“¤ì€ ê´œì°®ì§€ë§Œ ì•„ë¬´ ìƒê°ì—†ì´ ê¸´ ë¬¸ì¥ë“¤ì„ ì£¼-ìš± ë³µë¶™í•˜ë©´ ìˆœì‹ê°„ì— í……-ì¥ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ âš "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI APIë¥¼ ìˆ˜ì—…(ì‚¬ì´íŠ¸) ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  ë•Œ ì£¼ì˜í•  ì \n",
    "\n",
    "OepnAI íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë°©ë²•:\n",
    "```\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ê°ìì˜ ê³„ì •ì— ë¶€ì—¬ëœ ë¹„ë°€ keyë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤. [website](https://platform.openai.com/account/api-keys)  \n",
    "ì°¸ê³ ë¡œ API keyëŠ” ë°œê¸‰ë°›ì„ ë•Œë§Œ ì•Œë ¤ì£¼ê³  ê·¸ ì´í›„ë¡œëŠ” <span style='color:red'>ì ˆëŒ€ë¡œ ë‹¤ì‹œ í™•ì¸í•˜ì§€ ëª»í•˜ëŠ” êµ¬ì¡°</span>ì´ê¸° ë•Œë¬¸ì—, í˜¼ìë§Œ í™•ì¸ ê°€ëŠ¥í•œ ê³µê°„ì— ì˜ ê¸°ë¡í•´ë‘ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  \n",
    "\n",
    "ì²˜ìŒì— ë§ì”€ë“œë ¸ë˜ ê²ƒì²˜ëŸ¼ OpenAI API keyë¥¼ ë‹¤ë¥¸ ê³³ì— ì €ì¥í•´ë‘ê³  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì·¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "ì•„ë‹ˆë©´ ë³€ìˆ˜ì— ê°’ì„ ì§ì ‘ í• ë‹¹í•  ìˆ˜ë„ ìˆì£ :\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
